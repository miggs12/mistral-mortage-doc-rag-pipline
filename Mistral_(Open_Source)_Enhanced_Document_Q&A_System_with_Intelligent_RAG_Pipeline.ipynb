{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh6V_eigIL1i"
      },
      "source": [
        "# Local OCR-Enabled Document Q&A System (Mistral RAG)\n",
        "\n",
        "This project implements an end-to-end **Retrieval-Augmented Generation (RAG)** pipeline for semi-structured financial documents such as resumes, mortgage documents, and payslips.\n",
        "\n",
        "The system supports:\n",
        "- OCR for scanned PDFs\n",
        "- Page-level document classification\n",
        "- Semantic chunking and FAISS-based retrieval\n",
        "- Local LLM inference using **Mistral** via `llama.cpp`\n",
        "- Source-grounded answers with confidence estimation\n",
        "\n",
        "**Goal:** Reduce manual document review time while improving consistency and traceability of extracted information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## System Architecture\n",
        "\n",
        "This pipeline follows a modular, left-to-right architecture:\n",
        "\n",
        "PDF Upload\n",
        "‚Üí OCR & Text Extraction (PyMuPDF + Tesseract)\n",
        "‚Üí Page-Level Document Classification\n",
        "‚Üí Logical Document Grouping\n",
        "‚Üí Semantic Chunking (Sliding Window)\n",
        "‚Üí Embedding Generation (Local)\n",
        "‚Üí FAISS Vector Index\n",
        "‚Üí Top-K Retrieval\n",
        "‚Üí Mistral LLM Inference\n",
        "‚Üí Answer + Citations + Confidence\n",
        "\n",
        "\n",
        "Each stage is isolated to improve debuggability and system reliability.\n"
      ],
      "metadata": {
        "id": "rXgiM-vZH4vW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riBw_f9-IY4T"
      },
      "source": [
        "## Setup and Installation\n",
        "First install all necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MEXBca59DFk2",
        "outputId": "baa37167-d8c4-4d83-cb37-53aa04e4e659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu124\n",
            "Collecting llama-cpp-python\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.16-cu124/llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl (551.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m551.3/551.3 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
            "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
            "  Using cached numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting jinja2>=2.11.3 (from llama-cpp-python)\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
            "  Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Using cached numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.3.5\n",
            "    Uninstalling numpy-2.3.5:\n",
            "      Successfully uninstalled numpy-2.3.5\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.3\n",
            "    Uninstalling MarkupSafe-3.0.3:\n",
            "      Successfully uninstalled MarkupSafe-3.0.3\n",
            "  Attempting uninstall: diskcache\n",
            "    Found existing installation: diskcache 5.6.3\n",
            "    Uninstalling diskcache-5.6.3:\n",
            "      Successfully uninstalled diskcache-5.6.3\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: llama-cpp-python\n",
            "    Found existing installation: llama_cpp_python 0.3.16\n",
            "    Uninstalling llama_cpp_python-0.3.16:\n",
            "      Successfully uninstalled llama_cpp_python-0.3.16\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q gradio\n",
        "!pip install -q gradio_pdf\n",
        "!pip install -q pypdf PyPDF2 pymupdf\n",
        "!pip install -q faiss-cpu\n",
        "!pip install pandas\n",
        "\n",
        "# Install LlamaIndex packages for enhanced document processing\n",
        "!pip install -q llama-index\n",
        "!pip install -q llama-index-readers-file\n",
        "!pip install -q llama-index-embeddings-huggingface\n",
        "!pip install -q llama-index-vector-stores-faiss\n",
        "!pip install -q sentence-transformers\n",
        "\n",
        "!pip install llama-cpp-python --upgrade --force-reinstall \\\n",
        "    --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124\n",
        "\n",
        "!pip install -q pillow pytesseract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0qPyDllJbDH"
      },
      "source": [
        "## üîß Core Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrMYJ6havP1u",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Load the Local LLM model\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n",
        "    filename=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\",  # << correct filename\n",
        "    local_dir=\"/content\"\n",
        ")\n",
        "\n",
        "print(\"Model path:\", model_path)\n",
        "\n",
        "import os\n",
        "print(\"Size (GB):\", os.path.getsize(model_path) / (1024*1024*1024))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_QN4jpxJcGt",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from gradio_pdf import PDF\n",
        "import fitz  # PyMuPDF\n",
        "from PyPDF2 import PdfReader\n",
        "import numpy as np\n",
        "import faiss\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "import json\n",
        "from datetime import datetime\n",
        "import hashlib\n",
        "\n",
        "import io\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "\n",
        "from llama_index.core import Document as LI_Document, VectorStoreIndex, StorageContext\n",
        "from llama_index.core.schema import TextNode\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.vector_stores import MetadataFilters, MetadataFilter, FilterOperator\n",
        "from llama_cpp import Llama\n",
        "import os\n",
        "\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    !wget -c \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2-Q4_K_M.gguf\" -O {model_path}\n",
        "\n",
        "    print(f\"Model downloaded to {model_path}\")\n",
        "\n",
        "# Verify file exists and check size\n",
        "if os.path.exists(model_path):\n",
        "    print(f\"Model file exists. Size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB\")\n",
        "else:\n",
        "    print(\"Model file not found!\")\n",
        "\n",
        "try:\n",
        "    llm = Llama(\n",
        "      model_path=model_path,\n",
        "      n_ctx=4096,\n",
        "      n_threads=8,\n",
        "      n_gpu_layers=40, # use full GPU layers for max speed\n",
        "      n_batch=128,\n",
        "      use_mmap=True,\n",
        "      verbose=False,\n",
        "      use_mlock=False,\n",
        ")\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading LLM model: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Knln--L6OOSE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Load embedding model\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "EMBED_MODEL_PATH = \"/content/nomic-embed-text-v1.5.Q8_0.gguf\"\n",
        "\n",
        "if not os.path.exists(EMBED_MODEL_PATH):\n",
        "    !wget -q https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-gguf/resolve/main/nomic-embed-text-v1.5.Q8_0.gguf -O {EMBED_MODEL_PATH}\n",
        "    print(f\"Downloaded embedding model to {EMBED_MODEL_PATH}\")\n",
        "\n",
        "try:\n",
        "    embed_llm = SentenceTransformer(\n",
        "        \"nomic-ai/nomic-embed-text-v1.5\",\n",
        "        trust_remote_code=True,\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    )\n",
        "    print(\"Embedding model loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading embed model: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMLSudovJ5CV"
      },
      "source": [
        "Mistral and Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8GfHGgqJ4TL"
      },
      "outputs": [],
      "source": [
        "def mistral_generate(prompt: str, max_tokens: int = 512) -> str:\n",
        "    \"\"\"Wrapper to generate text from Mistral.\"\"\"\n",
        "    out = llm(\n",
        "        prompt,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "        stop=[\"</s>\"]\n",
        "    )\n",
        "    return out[\"choices\"][0][\"text\"].strip()\n",
        "\n",
        "def get_embedding(text: str) -> np.ndarray:\n",
        "    \"\"\"Get a float32 embedding vector for a given text using the embedding model.\"\"\"\n",
        "    res = embed_llm.create_embedding(text)\n",
        "    vec = res[\"data\"][0][\"embedding\"]\n",
        "    return np.array(vec, dtype=\"float32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9FtUNVZJjnz"
      },
      "source": [
        "## Data Structures for Enhanced Document Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiN5Ydr1Jpio"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class PageInfo:\n",
        "    \"\"\"Stores information about a single page\"\"\"\n",
        "    page_num: int\n",
        "    text: str\n",
        "    doc_type: Optional[str] = None\n",
        "    page_in_doc: int = 0\n",
        "\n",
        "@dataclass\n",
        "class LogicalDocument:\n",
        "    \"\"\"Represents a logical document within a PDF\"\"\"\n",
        "    doc_id: str\n",
        "    doc_type: str\n",
        "    page_start: int\n",
        "    page_end: int\n",
        "    text: str\n",
        "    chunks: List[Dict] = None\n",
        "\n",
        "@dataclass\n",
        "class ChunkMetadata:\n",
        "    \"\"\"Rich metadata for each chunk\"\"\"\n",
        "    chunk_id: str\n",
        "    doc_id: str\n",
        "    doc_type: str\n",
        "    chunk_index: int\n",
        "    page_start: int\n",
        "    page_end: int\n",
        "    text: str\n",
        "    embedding: Optional[np.ndarray] = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMN46qYfJrz9"
      },
      "source": [
        "## Document Intelligence Functions\n",
        "These functions handle document classification and boundary detection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSkSE_xPIZQK"
      },
      "outputs": [],
      "source": [
        "def classify_document_type(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Hybrid document classifier.\n",
        "    Uses rules first, LLM second.\n",
        "    \"\"\"\n",
        "    if not text or len(text.strip()) < 50:\n",
        "        return \"Other\"\n",
        "\n",
        "    t = text.lower().replace(\" \", \"\")\n",
        "\n",
        "    # STRONG PAYSLIP SIGNALS\n",
        "    if (\n",
        "        \"netpay\" in t\n",
        "        or \"grosspay\" in t\n",
        "        or \"basic\" in t\n",
        "        or \"deduction\" in t\n",
        "        or \"employeeid\" in t\n",
        "        or \"paydate\" in t\n",
        "    ):\n",
        "        return \"Payslip\"\n",
        "\n",
        "    # STRONG MORTGAGE/LOAN SIGNALS\n",
        "    if (\n",
        "        \"loanestimate\" in t\n",
        "        or \"closingdisclosure\" in t\n",
        "        or \"interestrate\" in t\n",
        "        or \"borrower\" in t\n",
        "        or \"loanamount\" in t\n",
        "        or \"principal\" in t\n",
        "    ):\n",
        "        return \"Mortgage Document\"\n",
        "\n",
        "    # STRONG INVOICE SIGNALS\n",
        "    if (\n",
        "        \"invoice\" in t\n",
        "        or \"subtotal\" in t\n",
        "        or \"totaldue\" in t\n",
        "        or \"billto\" in t\n",
        "    ):\n",
        "        return \"Invoice\"\n",
        "\n",
        "    # ‚úÖ RESUME SIGNALS (ADD THIS)\n",
        "    if (\n",
        "        \"experience\" in t\n",
        "        or \"education\" in t\n",
        "        or \"skills\" in t\n",
        "        or \"workexperience\" in t\n",
        "        or \"professionalexperience\" in t\n",
        "        or \"summary\" in t\n",
        "        or \"projects\" in t\n",
        "        or \"linkedin.com\" in t\n",
        "    ):\n",
        "        return \"Resume\"\n",
        "\n",
        "    return \"Other\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz_YivbFJ10K"
      },
      "source": [
        "## Advanced PDF Processing Pipeline\n",
        "Enhanced PDF processing pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper\n",
        "def cheap_same_doc_heuristic(prev_text: str, curr_text: str) -> bool:\n",
        "    prev = prev_text.lower().replace(\" \", \"\")\n",
        "    curr = curr_text.lower().replace(\" \", \"\")\n",
        "\n",
        "    key_markers = [\n",
        "        \"employee\",\n",
        "        \"netpay\",\n",
        "        \"grosspay\",\n",
        "        \"basic\",\n",
        "        \"deduction\",\n",
        "        \"paydate\",\n",
        "        \"empid\"\n",
        "    ]\n",
        "\n",
        "    matches = sum(1 for k in key_markers if k in prev and k in curr)\n"
      ],
      "metadata": {
        "id": "a2rB9x2ntgp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_document_boundary(prev_text, curr_text, current_doc_type):\n",
        "    \"\"\"\n",
        "    Returns True if pages belong to the SAME document.\n",
        "    \"\"\"\n",
        "\n",
        "    if not prev_text.strip() or not curr_text.strip():\n",
        "        return True\n",
        "\n",
        "    prev_type = classify_document_type(prev_text)\n",
        "    curr_type = classify_document_type(curr_text)\n",
        "\n",
        "    # If the document type changes, force a split\n",
        "    if prev_type != curr_type:\n",
        "        return False\n",
        "\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "DQ0kyWq2KvQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIBKFsndJ5TT"
      },
      "outputs": [],
      "source": [
        "def extract_and_analyze_pdf(pdf_file) -> Tuple[List[PageInfo], List[LogicalDocument]]:\n",
        "    \"\"\"\n",
        "    Extract text from PDF and group pages into logical documents.\n",
        "    \"\"\"\n",
        "    print(\"üìñ Starting PDF extraction and analysis...\")\n",
        "\n",
        "    if isinstance(pdf_file, dict) and \"content\" in pdf_file:\n",
        "        doc = fitz.open(stream=pdf_file[\"content\"], filetype=\"pdf\")\n",
        "    elif hasattr(pdf_file, \"read\"):\n",
        "        doc = fitz.open(stream=pdf_file.read(), filetype=\"pdf\")\n",
        "    else:\n",
        "        doc = fitz.open(pdf_file)\n",
        "\n",
        "    pages_info: List[PageInfo] = []\n",
        "    for i, page in enumerate(doc):\n",
        "        text = page.get_text()\n",
        "\n",
        "        if not text.strip():\n",
        "            print(f\"  Page {i}: No text found, attempting OCR...\")\n",
        "            try:\n",
        "                pix = page.get_pixmap()\n",
        "                img = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n",
        "                text = pytesseract.image_to_string(img)\n",
        "                print(f\"  Page {i}: OCR extracted {len(text)} characters\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Page {i}: OCR failed - {e}\")\n",
        "                text = \"\"\n",
        "\n",
        "        pages_info.append(PageInfo(page_num=i, text=text))\n",
        "\n",
        "    doc.close()\n",
        "\n",
        "    if not pages_info:\n",
        "        raise ValueError(\"No text could be extracted from PDF\")\n",
        "\n",
        "    print(f\"‚úÖ Extracted {len(pages_info)} pages\")\n",
        "\n",
        "    logical_docs: List[LogicalDocument] = []\n",
        "    current_doc_pages: List[PageInfo] = []\n",
        "    current_doc_type = None\n",
        "    doc_counter = 0\n",
        "\n",
        "    print(\"üß† Analyzing document structure...\")\n",
        "\n",
        "    for i, page_info in enumerate(pages_info):\n",
        "        if i == 0:\n",
        "            # Always classify first page ONCE\n",
        "            current_doc_type = classify_document_type(page_info.text)\n",
        "            page_info.doc_type = current_doc_type\n",
        "            page_info.page_in_doc = 0\n",
        "            current_doc_pages = [page_info]\n",
        "            print(f\"  Page {i}: New document detected - {current_doc_type}\")\n",
        "            continue\n",
        "\n",
        "        prev_text = pages_info[i - 1].text\n",
        "\n",
        "        # Decision order: heuristic ‚Üí LLM\n",
        "        if cheap_same_doc_heuristic(prev_text, page_info.text):\n",
        "            is_same = True\n",
        "        else:\n",
        "            is_same = detect_document_boundary(prev_text, page_info.text, current_doc_type)\n",
        "\n",
        "        if is_same:\n",
        "            page_info.doc_type = current_doc_type\n",
        "            page_info.page_in_doc = len(current_doc_pages)\n",
        "            current_doc_pages.append(page_info)\n",
        "        else:\n",
        "            # Finalize previous document\n",
        "            logical_docs.append(\n",
        "                LogicalDocument(\n",
        "                    doc_id=f\"doc_{doc_counter}\",\n",
        "                    doc_type=current_doc_type,\n",
        "                    page_start=current_doc_pages[0].page_num,\n",
        "                    page_end=current_doc_pages[-1].page_num,\n",
        "                    text=\"\\n\\n\".join(p.text for p in current_doc_pages)\n",
        "                )\n",
        "            )\n",
        "            doc_counter += 1\n",
        "\n",
        "            # Start new document\n",
        "            current_doc_type = classify_document_type(page_info.text)\n",
        "            page_info.doc_type = current_doc_type\n",
        "            page_info.page_in_doc = 0\n",
        "            current_doc_pages = [page_info]\n",
        "            print(f\"  Page {i}: New document detected - {current_doc_type}\")\n",
        "\n",
        "    # Final document\n",
        "    if current_doc_pages:\n",
        "        logical_docs.append(\n",
        "            LogicalDocument(\n",
        "                doc_id=f\"doc_{doc_counter}\",\n",
        "                doc_type=current_doc_type,\n",
        "                page_start=current_doc_pages[0].page_num,\n",
        "                page_end=current_doc_pages[-1].page_num,\n",
        "                text=\"\\n\\n\".join(p.text for p in current_doc_pages)\n",
        "            )\n",
        "        )\n",
        "\n",
        "    print(f\"‚úÖ Identified {len(logical_docs)} logical documents\")\n",
        "    for ld in logical_docs:\n",
        "        print(f\"   - {ld.doc_type}: Pages {ld.page_start}-{ld.page_end}\")\n",
        "\n",
        "    return pages_info, logical_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM89pqZjK_mK"
      },
      "source": [
        "## Intelligent Chunking with Metadata Preservation\n",
        "We'll provide two chunking approaches - our custom implementation and LlamaIndex's built-in capabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6jI6IMnLCX1"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document as LI_Document\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "def chunk_document_with_metadata(\n",
        "    logical_doc: LogicalDocument,\n",
        "    chunk_size: int = 300,\n",
        "    overlap: int = 50\n",
        ") -> List[ChunkMetadata]:\n",
        "    \"\"\"\n",
        "    Structure-aware chunking for financial documents.\n",
        "    Preserves tables and logical blocks.\n",
        "    \"\"\"\n",
        "    text = logical_doc.text.strip()\n",
        "\n",
        "    sections = []\n",
        "    buffer = []\n",
        "\n",
        "    for line in text.splitlines():\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            if buffer:\n",
        "                sections.append(\" \".join(buffer))\n",
        "                buffer = []\n",
        "        else:\n",
        "            buffer.append(line)\n",
        "\n",
        "    if buffer:\n",
        "        sections.append(\" \".join(buffer))\n",
        "\n",
        "    chunks_metadata: List[ChunkMetadata] = []\n",
        "    chunk_id = 0\n",
        "\n",
        "    for section in sections:\n",
        "        words = section.split()\n",
        "\n",
        "        if len(words) <= chunk_size:\n",
        "            chunks_metadata.append(\n",
        "                ChunkMetadata(\n",
        "                    chunk_id=f\"{logical_doc.doc_id}_chunk_{chunk_id}\",\n",
        "                    doc_id=logical_doc.doc_id,\n",
        "                    doc_type=logical_doc.doc_type,\n",
        "                    chunk_index=chunk_id,\n",
        "                    page_start=logical_doc.page_start,\n",
        "                    page_end=logical_doc.page_end,\n",
        "                    text=section,\n",
        "                )\n",
        "            )\n",
        "            chunk_id += 1\n",
        "            continue\n",
        "\n",
        "        stride = chunk_size - overlap\n",
        "        for start in range(0, len(words), stride):\n",
        "            chunk_text = \" \".join(words[start:start + chunk_size])\n",
        "\n",
        "            chunks_metadata.append(\n",
        "                ChunkMetadata(\n",
        "                    chunk_id=f\"{logical_doc.doc_id}_chunk_{chunk_id}\",\n",
        "                    doc_id=logical_doc.doc_id,\n",
        "                    doc_type=logical_doc.doc_type,\n",
        "                    chunk_index=chunk_id,\n",
        "                    page_start=logical_doc.page_start,\n",
        "                    page_end=logical_doc.page_end,\n",
        "                    text=chunk_text,\n",
        "                )\n",
        "            )\n",
        "            chunk_id += 1\n",
        "\n",
        "            if start + chunk_size >= len(words):\n",
        "                break\n",
        "\n",
        "    return chunks_metadata\n",
        "\n",
        "def chunk_with_llama_index(\n",
        "    logical_doc: LogicalDocument,\n",
        "    chunk_size: int = 400,\n",
        "    chunk_overlap: int = 80\n",
        ") -> List[ChunkMetadata]:\n",
        "    \"\"\"\n",
        "    Sentence-based chunking using LlamaIndex.\n",
        "    Better semantic coherence, slightly slower.\n",
        "    \"\"\"\n",
        "    li_doc = LI_Document(\n",
        "        text=logical_doc.text,\n",
        "        metadata={\n",
        "            \"doc_id\": logical_doc.doc_id,\n",
        "            \"doc_type\": logical_doc.doc_type,\n",
        "            \"page_start\": logical_doc.page_start,\n",
        "            \"page_end\": logical_doc.page_end,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    splitter = SentenceSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        paragraph_separator=\"\\n\\n\",\n",
        "    )\n",
        "\n",
        "    nodes = splitter.get_nodes_from_documents([li_doc])\n",
        "\n",
        "    chunks: List[ChunkMetadata] = []\n",
        "    for i, node in enumerate(nodes):\n",
        "        chunks.append(\n",
        "            ChunkMetadata(\n",
        "                chunk_id=f\"{logical_doc.doc_id}_chunk_{i}\",\n",
        "                doc_id=logical_doc.doc_id,\n",
        "                doc_type=logical_doc.doc_type,\n",
        "                chunk_index=i,\n",
        "                page_start=node.metadata.get(\"page_start\", logical_doc.page_start),\n",
        "                page_end=node.metadata.get(\"page_end\", logical_doc.page_end),\n",
        "                text=node.text,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxKCas_YstTw"
      },
      "outputs": [],
      "source": [
        "# REQUIRED: Define process_all_documents (missing in kernel)\n",
        "\n",
        "def process_all_documents(\n",
        "    logical_docs,\n",
        "    use_llama_index: bool = True\n",
        "):\n",
        "    \"\"\"\n",
        "    Chunk all logical documents and attach metadata.\n",
        "    Returns a flat list of ChunkMetadata.\n",
        "    \"\"\"\n",
        "    all_chunks = []\n",
        "\n",
        "    for logical_doc in logical_docs:\n",
        "        if use_llama_index:\n",
        "            chunks = chunk_with_llama_index(logical_doc)\n",
        "        else:\n",
        "            chunks = chunk_document_with_metadata(logical_doc)\n",
        "\n",
        "        logical_doc.chunks = chunks\n",
        "        all_chunks.extend(chunks)\n",
        "\n",
        "        print(f\"üìÑ {logical_doc.doc_type}: Created {len(chunks)} chunks\")\n",
        "\n",
        "    return all_chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RESzcztYLEgS"
      },
      "source": [
        "## Query Routing and Intelligent Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tqd7HSzPLMVJ"
      },
      "outputs": [],
      "source": [
        "def predict_query_document_type(query: str) -> Tuple[str, float]:\n",
        "    \"\"\"\n",
        "    Predict which document type is most likely to contain the answer using Mistral.\n",
        "    Returns (type, confidence).\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "You are a query routing assistant.\n",
        "\n",
        "Analyze this query and predict which document type would most likely contain the answer.\n",
        "\n",
        "Query: \"{query}\"\n",
        "\n",
        "Choose exactly ONE of:\n",
        "- Resume\n",
        "- Contract\n",
        "- Mortgage Contract\n",
        "- Invoice\n",
        "- Pay Slip\n",
        "- Lender Fee Sheet\n",
        "- Land Deed\n",
        "- Bank Statement\n",
        "- Tax Document\n",
        "- Insurance\n",
        "- Report\n",
        "- Letter\n",
        "- Form\n",
        "- ID Document\n",
        "- Medical\n",
        "- Other\n",
        "\n",
        "Respond in JSON ONLY, like:\n",
        "{{\"type\": \"Invoice\", \"confidence\": 0.87}}\n",
        "\"\"\"\n",
        "    try:\n",
        "        raw = mistral_generate(prompt, max_tokens=128)\n",
        "        # Try to find JSON inside the response (in case of extra text)\n",
        "        start = raw.find(\"{\")\n",
        "        end = raw.rfind(\"}\") + 1\n",
        "        if start != -1 and end != -1:\n",
        "            raw_json = raw[start:end]\n",
        "        else:\n",
        "            raw_json = raw\n",
        "\n",
        "        result = json.loads(raw_json)\n",
        "        return result.get(\"type\", \"Other\"), float(result.get(\"confidence\", 0.5))\n",
        "    except Exception as e:\n",
        "        print(f\"Query routing error: {e}\")\n",
        "        return \"Other\", 0.0\n",
        "\n",
        "class IntelligentRetriever:\n",
        "    \"\"\"\n",
        "    Advanced retrieval system with metadata filtering and query routing.\n",
        "    Uses local embedding model + FAISS.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.index = None\n",
        "        self.chunks_metadata: List[ChunkMetadata] = []\n",
        "        self.doc_type_indices = {}\n",
        "        self.total_queries = 0\n",
        "        self.cache_hits = 0  # Placeholder if you add caching later\n",
        "\n",
        "    def build_indices(self, chunks_metadata: List[ChunkMetadata]):\n",
        "        print(\"üî® Building vector indices...\")\n",
        "        self.chunks_metadata = chunks_metadata\n",
        "\n",
        "        # Compute embeddings\n",
        "        texts = [\n",
        "          f\"MORTGAGE DOCUMENT.\\n\"\n",
        "          f\"Type: {chunk.doc_type}.\\n\"\n",
        "          f\"Pages: {chunk.page_start}-{chunk.page_end}.\\n\"\n",
        "          f\"Content: {chunk.text[:600]}\"\n",
        "          for chunk in chunks_metadata\n",
        "        ]\n",
        "\n",
        "        embeddings = []\n",
        "        for t in texts:\n",
        "            embeddings.append(get_embedding(t))\n",
        "        embeddings = np.vstack(embeddings).astype(\"float32\")\n",
        "\n",
        "        for i, chunk in enumerate(chunks_metadata):\n",
        "            chunk.embedding = embeddings[i]\n",
        "\n",
        "        dim = embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatL2(dim)\n",
        "        self.index.add(embeddings)\n",
        "\n",
        "        doc_types = set(chunk.doc_type for chunk in chunks_metadata)\n",
        "        for doc_type in doc_types:\n",
        "            type_indices = [i for i, chunk in enumerate(chunks_metadata)\n",
        "                            if chunk.doc_type == doc_type]\n",
        "            if type_indices:\n",
        "                type_embeddings = embeddings[type_indices]\n",
        "                type_index = faiss.IndexFlatL2(dim)\n",
        "                type_index.add(type_embeddings)\n",
        "                self.doc_type_indices[doc_type] = {\n",
        "                    \"index\": type_index,\n",
        "                    \"mapping\": type_indices\n",
        "                }\n",
        "\n",
        "        print(f\"‚úÖ Indexed {len(chunks_metadata)} chunks across {len(doc_types)} document types\")\n",
        "\n",
        "    def retrieve(self, query: str, k: int = 4,\n",
        "                 filter_doc_type: Optional[str] = None,\n",
        "                 auto_route: bool = True) -> List[Tuple[ChunkMetadata, float]]:\n",
        "        self.total_queries += 1\n",
        "\n",
        "        query_vec = get_embedding(query)\n",
        "        query_embedding = query_vec.reshape(1, -1).astype(\"float32\")\n",
        "\n",
        "        if filter_doc_type and filter_doc_type in self.doc_type_indices:\n",
        "            type_data = self.doc_type_indices[filter_doc_type]\n",
        "            D, I = type_data[\"index\"].search(query_embedding, k)\n",
        "            chunk_indices = [type_data[\"mapping\"][i] for i in I[0]]\n",
        "            distances = D[0]\n",
        "        elif auto_route:\n",
        "            predicted_type, confidence = predict_query_document_type(query)\n",
        "            print(f\"üéØ Query routed to: {predicted_type} (confidence: {confidence:.2f})\")\n",
        "\n",
        "            if confidence > 0.7 and predicted_type in self.doc_type_indices:\n",
        "                type_data = self.doc_type_indices[predicted_type]\n",
        "                D, I = type_data[\"index\"].search(query_embedding, k)\n",
        "                chunk_indices = [type_data[\"mapping\"][i] for i in I[0]]\n",
        "                distances = D[0]\n",
        "            else:\n",
        "                D, I = self.index.search(query_embedding, k)\n",
        "                chunk_indices = I[0]\n",
        "                distances = D[0]\n",
        "        else:\n",
        "            D, I = self.index.search(query_embedding, k)\n",
        "            chunk_indices = I[0]\n",
        "            distances = D[0]\n",
        "\n",
        "        max_dist = max(distances) if len(distances) > 0 else 1.0\n",
        "        scores = [(max_dist - d) / max_dist for d in distances]\n",
        "\n",
        "        results = [(self.chunks_metadata[i], scores[idx])\n",
        "                   for idx, i in enumerate(chunk_indices)]\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT4Dws--LN2F"
      },
      "source": [
        "## Enhanced Answer Generation with Source Attribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IITpvhTeLRsg"
      },
      "outputs": [],
      "source": [
        "def generate_answer_with_sources(query: str,\n",
        "                                 retrieved_chunks: List[Tuple[ChunkMetadata, float]]) -> Dict:\n",
        "    if not retrieved_chunks:\n",
        "        return {\n",
        "            \"answer\": \"I couldn't find relevant information to answer your question.\",\n",
        "            \"sources\": [],\n",
        "            \"confidence\": 0.0\n",
        "        }\n",
        "\n",
        "    context_parts = []\n",
        "    sources = []\n",
        "\n",
        "    MAX_CHARS = 500\n",
        "    MAX_CHUNKS = 3\n",
        "    for chunk_meta, score in retrieved_chunks[:MAX_CHUNKS]:\n",
        "        context_parts.append(\n",
        "            f\"[From {chunk_meta.doc_type}, Pages {chunk_meta.page_start}-{chunk_meta.page_end}]\"\n",
        "            )\n",
        "        context_parts.append(chunk_meta.text[:MAX_CHARS])\n",
        "        context_parts.append(\"\")\n",
        "\n",
        "        sources.append({\n",
        "            \"doc_type\": chunk_meta.doc_type,\n",
        "            \"pages\": f\"{chunk_meta.page_start}-{chunk_meta.page_end}\",\n",
        "            \"relevance\": f\"{score:.2%}\",\n",
        "            \"preview\": chunk_meta.text[:100] + \"...\"\n",
        "        })\n",
        "\n",
        "    context = \"\\n\".join(context_parts)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a helpful AI assistant. Use the provided context to answer the question.\n",
        "Be specific and cite which document type and pages support your answer.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Instructions:\n",
        "1. Answer based ONLY on the provided context.\n",
        "2. Mention which document type(s) contain the information.\n",
        "3. Be concise but complete.\n",
        "4. If the context doesn't contain enough information, say so.\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    try:\n",
        "        answer = mistral_generate(prompt, max_tokens=512).strip()\n",
        "        avg_score = sum(s for _, s in retrieved_chunks) / len(retrieved_chunks)\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"sources\": sources,\n",
        "            \"confidence\": avg_score,\n",
        "            \"chunks_used\": len(retrieved_chunks)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Answer generation error: {e}\")\n",
        "        return {\n",
        "            \"answer\": f\"Error generating answer: {str(e)}\",\n",
        "            \"sources\": sources,\n",
        "            \"confidence\": 0.0\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbvV4zsCLTEu"
      },
      "source": [
        "## Enhanced Document Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyy5OLSTLViM"
      },
      "outputs": [],
      "source": [
        "class EnhancedDocumentStore:\n",
        "    def __init__(self):\n",
        "        self.pages_info: List[PageInfo] = []\n",
        "        self.logical_docs: List[LogicalDocument] = []\n",
        "        self.chunks_metadata: List[ChunkMetadata] = []\n",
        "        self.retriever = IntelligentRetriever()\n",
        "        self.is_ready = False\n",
        "        self.processing_stats = {}\n",
        "        self.filename = None\n",
        "\n",
        "    def process_pdf(self, pdf_file, filename: str = \"document.pdf\"):\n",
        "        self.filename = filename\n",
        "        self.is_ready = False\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        try:\n",
        "            self.pages_info, self.logical_docs = extract_and_analyze_pdf(pdf_file)\n",
        "            self.chunks_metadata = process_all_documents(self.logical_docs)\n",
        "            self.retriever.build_indices(self.chunks_metadata)\n",
        "\n",
        "            process_time = (datetime.now() - start_time).total_seconds()\n",
        "            self.processing_stats = {\n",
        "                \"filename\": filename,\n",
        "                \"total_pages\": len(self.pages_info),\n",
        "                \"documents_found\": len(self.logical_docs),\n",
        "                \"total_chunks\": len(self.chunks_metadata),\n",
        "                \"document_types\": list(set(doc.doc_type for doc in self.logical_docs)),\n",
        "                \"processing_time\": f\"{process_time:.1f}s\"\n",
        "            }\n",
        "\n",
        "            self.is_ready = True\n",
        "            return True, self.processing_stats\n",
        "        except Exception as e:\n",
        "            return False, {\"error\": str(e)}\n",
        "\n",
        "    def query(self, question: str, filter_type: Optional[str] = None,\n",
        "              auto_route: bool = True, k: int = 4) -> Dict:\n",
        "        if not self.is_ready:\n",
        "            return {\n",
        "                \"answer\": \"Please upload and process a PDF first.\",\n",
        "                \"sources\": [],\n",
        "                \"confidence\": 0.0\n",
        "            }\n",
        "\n",
        "        retrieved = self.retriever.retrieve( # made K=3 because the tokens exceed 4096\n",
        "            question, k=3,\n",
        "            filter_doc_type=filter_type,\n",
        "            auto_route=auto_route and filter_type is None\n",
        "        )\n",
        "\n",
        "        result = generate_answer_with_sources(question, retrieved)\n",
        "        result[\"filter_used\"] = filter_type or (\"auto\" if auto_route else \"none\")\n",
        "        return result\n",
        "\n",
        "    def get_document_structure(self) -> List[Dict]:\n",
        "        if not self.logical_docs:\n",
        "            return []\n",
        "\n",
        "        structure = []\n",
        "        for doc in self.logical_docs:\n",
        "            structure.append({\n",
        "                \"id\": doc.doc_id,\n",
        "                \"type\": doc.doc_type,\n",
        "                \"pages\": f\"{doc.page_start + 1}-{doc.page_end + 1}\",\n",
        "                \"chunks\": len(doc.chunks) if doc.chunks else 0,\n",
        "                \"preview\": doc.text[:200] + \"...\" if len(doc.text) > 200 else doc.text\n",
        "            })\n",
        "        return structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbeCC7ItLWpA"
      },
      "source": [
        "## Gradio Interface with Enhanced Features\n",
        "Gradio interface:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7wXGcnJN6rf"
      },
      "outputs": [],
      "source": [
        "doc_store = EnhancedDocumentStore()\n",
        "\n",
        "def process_pdf_handler(pdf_file):\n",
        "    if pdf_file is None:\n",
        "        return \"‚ö†Ô∏è Please upload a PDF file\", None, gr.update(choices=[\"All\"]), None\n",
        "\n",
        "    success, stats = doc_store.process_pdf(\n",
        "        pdf_file,\n",
        "        filename=getattr(pdf_file, \"name\", \"document.pdf\")\n",
        "    )\n",
        "\n",
        "    if success:\n",
        "        status_msg = f\"\"\"\n",
        "‚úÖ **Successfully Processed**\n",
        "- File: {stats['filename']}\n",
        "- Pages: {stats['total_pages']}\n",
        "- Docs Found: {stats['documents_found']}\n",
        "- Chunks: {stats['total_chunks']}\n",
        "\"\"\"\n",
        "\n",
        "        structure = doc_store.get_document_structure()\n",
        "        structure_display = \"\\n\".join([\n",
        "            f\"‚Ä¢ {doc['type']} (Pages {doc['pages']})\"\n",
        "            for doc in structure\n",
        "        ])\n",
        "\n",
        "        doc_types = [\"All\"] + stats[\"document_types\"]\n",
        "\n",
        "        # ‚úÖ RETURN PDF PATH HERE\n",
        "        return status_msg, structure_display, gr.update(choices=doc_types, value=\"All\")\n",
        "\n",
        "    return \"‚ùå Processing failed\", None, gr.update(choices=[\"All\"]), None\n",
        "\n",
        "def chat_handler(message, history, doc_filter, auto_route, num_chunks):\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    if not doc_store.is_ready:\n",
        "        history.append(\n",
        "            {\"role\": \"assistant\", \"content\": \"üìö Please upload and process a PDF document first.\"}\n",
        "        )\n",
        "        return history\n",
        "\n",
        "    filter_type = None if doc_filter == \"All\" else doc_filter\n",
        "    result = doc_store.query(\n",
        "        message,\n",
        "        filter_type=filter_type,\n",
        "        auto_route=auto_route and filter_type is None,\n",
        "        k=num_chunks\n",
        "    )\n",
        "\n",
        "    response = result[\"answer\"] + \"\\n\\n\"\n",
        "    if result[\"sources\"]:\n",
        "        response += \"üìç Sources:\\n\"\n",
        "        for src in result[\"sources\"]:\n",
        "            response += f\"- {src['doc_type']} (Pages {src['pages']})\\n\"\n",
        "\n",
        "    response += f\"\\nConfidence: {result['confidence']:.1%}\"\n",
        "\n",
        "    history.append({\"role\": \"user\", \"content\": message})\n",
        "    history.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "    return history\n",
        "\n",
        "def create_interface():\n",
        "    with gr.Blocks(title=\"Enhanced Document Q&A (Mistral RAG)\") as demo:\n",
        "        gr.Markdown(\"\"\"\n",
        "# üöÄ Enhanced Document Q&A System (Mistral RAG)\n",
        "### Intelligent Multi-Document Analysis with Local RAG Pipeline\n",
        "\"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                pdf_input = gr.File(\n",
        "                    label=\"üìÑ Upload PDF\",\n",
        "                    #interactive=True,\n",
        "                    #height=600,\n",
        "                    file_types=[\".pdf\"],\n",
        "                    type=\"filepath\"\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    process_btn = gr.Button(\"üîÑ Process Document\", variant=\"primary\", scale=2)\n",
        "                    clear_all_btn = gr.Button(\"üóëÔ∏è Clear All\", variant=\"secondary\", scale=1)\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### üìä Document Info\")\n",
        "                status_output = gr.Markdown(value=\"‚è≥ Waiting for PDF upload...\")\n",
        "                structure_output = gr.Markdown(value=\"\", label=\"Document Structure\")\n",
        "\n",
        "                gr.Markdown(\"### ‚öôÔ∏è Settings\")\n",
        "                doc_filter = gr.Dropdown(\n",
        "                    choices=[\"All\"],\n",
        "                    value=\"All\",\n",
        "                    label=\"üè∑Ô∏è Document Type Filter\",\n",
        "                    info=\"Filter search to specific document type\"\n",
        "                )\n",
        "                auto_route = gr.Checkbox(\n",
        "                    value=True,\n",
        "                    label=\"üéØ Auto-Route Queries\",\n",
        "                    info=\"Automatically detect relevant document type\"\n",
        "                )\n",
        "                num_chunks = gr.Slider(\n",
        "                    minimum=1,\n",
        "                    maximum=10,\n",
        "                    value=4,\n",
        "                    step=1,\n",
        "                    label=\"üìä Chunks to Retrieve\"\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"### üí¨ Ask Questions\")\n",
        "                chatbot = gr.Chatbot(\n",
        "                    label=\"Conversation\",\n",
        "                    height=500,\n",
        "                    #elem_id=\"chatbot\",\n",
        "                    #show_label=False\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    msg_input = gr.Textbox(\n",
        "                        label=\"Ask a question\",\n",
        "                        placeholder=\"e.g., What are the payment terms? What is the total amount?\",\n",
        "                        scale=4,\n",
        "                        show_label=False\n",
        "                    )\n",
        "                    send_btn = gr.Button(\"üì§ Send\", scale=1, variant=\"primary\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    clear_chat_btn = gr.Button(\"üóëÔ∏è Clear Chat\", size=\"sm\", scale=1)\n",
        "                    example_btn1 = gr.Button(\"üìù What's the summary?\", size=\"sm\", scale=1)\n",
        "                    example_btn2 = gr.Button(\"üí∞ Find amounts\", size=\"sm\", scale=1)\n",
        "\n",
        "        with gr.Row():\n",
        "            status_bar = gr.Markdown(\n",
        "                value=\"**Status:** Ready | **Documents:** 0 | **Chunks:** 0 | **Cache Hits:** 0/0\",\n",
        "                elem_id=\"status_bar\"\n",
        "            )\n",
        "\n",
        "        def update_status_bar():\n",
        "            if doc_store.is_ready:\n",
        "                stats = doc_store.processing_stats\n",
        "                cache_rate = 0\n",
        "                if doc_store.retriever.total_queries > 0:\n",
        "                    cache_rate = (doc_store.retriever.cache_hits / doc_store.retriever.total_queries) * 100\n",
        "                return f\"**Status:** ‚úÖ Ready | **Documents:** {stats.get('documents_found', 0)} | **Chunks:** {stats.get('total_chunks', 0)} | **Cache Rate:** {cache_rate:.0f}%\"\n",
        "            return \"**Status:** Ready | **Documents:** 0 | **Chunks:** 0 | **Cache Hits:** 0/0\"\n",
        "\n",
        "        def clear_all():\n",
        "            doc_store.pages_info = []\n",
        "            doc_store.logical_docs = []\n",
        "            doc_store.chunks_metadata = []\n",
        "            doc_store.retriever = IntelligentRetriever()\n",
        "            doc_store.is_ready = False\n",
        "            return (\n",
        "                None,                              # pdf_input\n",
        "                \"‚è≥ Waiting for PDF upload...\",     # status_output\n",
        "                \"\",                                # structure_output\n",
        "                gr.update(choices=[\"All\"], value=\"All\"),  # doc_filter\n",
        "                [],                                # chatbot\n",
        "                \"\",                                # msg_input\n",
        "                update_status_bar()                # status_bar\n",
        "            )\n",
        "\n",
        "        def process_pdf_with_status(pdf_file):\n",
        "            status, structure, filter_update = process_pdf_handler(pdf_file)\n",
        "            status_bar_text = update_status_bar()\n",
        "            return status, structure, filter_update, status_bar_text\n",
        "\n",
        "        def chat_with_status(message, history, doc_filter, auto_route, num_chunks):\n",
        "            new_history = chat_handler(message, history, doc_filter, auto_route, num_chunks)\n",
        "            status_bar_text = update_status_bar()\n",
        "            return new_history, status_bar_text\n",
        "\n",
        "        def ask_summary(history):\n",
        "            return chat_handler(\n",
        "                \"Can you provide a summary of the main points in this document?\",\n",
        "                history, doc_filter.value, auto_route.value, num_chunks.value\n",
        "            )\n",
        "\n",
        "        def ask_amounts(history):\n",
        "            return chat_handler(\n",
        "                \"What are all the monetary amounts or financial figures mentioned?\",\n",
        "                history, doc_filter.value, auto_route.value, num_chunks.value\n",
        "            )\n",
        "\n",
        "        process_btn.click(\n",
        "          fn=process_pdf_with_status,\n",
        "          inputs=[pdf_input],\n",
        "          outputs=[\n",
        "            status_output,\n",
        "            structure_output,\n",
        "            doc_filter,\n",
        "            status_bar,\n",
        "          ]\n",
        "        )\n",
        "\n",
        "        clear_all_btn.click(\n",
        "            fn=clear_all,\n",
        "            outputs=[pdf_input, status_output, structure_output, doc_filter,\n",
        "                     chatbot, msg_input, status_bar]\n",
        "        )\n",
        "\n",
        "        msg_input.submit(\n",
        "            fn=chat_with_status,\n",
        "            inputs=[msg_input, chatbot, doc_filter, auto_route, num_chunks],\n",
        "            outputs=[chatbot, status_bar]\n",
        "        ).then(lambda: \"\", outputs=[msg_input])\n",
        "\n",
        "        send_btn.click(\n",
        "            fn=chat_with_status,\n",
        "            inputs=[msg_input, chatbot, doc_filter, auto_route, num_chunks],\n",
        "            outputs=[chatbot, status_bar]\n",
        "        ).then(lambda: \"\", outputs=[msg_input])\n",
        "\n",
        "        clear_chat_btn.click(lambda: [], outputs=[chatbot])\n",
        "\n",
        "    return demo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually process PDF for metrics\n",
        "# Replace with real file path\n",
        "\n",
        "PDF_PATH = \"Blob File Sample.pdf\"\n",
        "\n",
        "success, stats = doc_store.process_pdf(PDF_PATH)\n",
        "\n",
        "print(\"Processed:\", success)\n",
        "print(\"Stats:\", stats)\n",
        "print(\"FAISS index exists:\", doc_store.retriever.index is not None)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XkDNiUxztEcR",
        "outputId": "5b8c5948-a350-4a76-d00c-32f8b17f5def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìñ Starting PDF extraction and analysis...\n",
            "‚úÖ Extracted 4 pages\n",
            "üß† Analyzing document structure...\n",
            "  Page 0: New document detected - Resume\n",
            "  Page 1: New document detected - Mortgage Document\n",
            "  Page 2: New document detected - Payslip\n",
            "‚úÖ Identified 3 logical documents\n",
            "   - Resume: Pages 0-0\n",
            "   - Mortgage Document: Pages 1-1\n",
            "   - Payslip: Pages 2-3\n",
            "üìÑ Resume: Created 1 chunks\n",
            "üìÑ Mortgage Document: Created 3 chunks\n",
            "üìÑ Payslip: Created 1 chunks\n",
            "üî® Building vector indices...\n",
            "Processed: False\n",
            "Stats: {'error': \"name 'embed_llm' is not defined\"}\n",
            "FAISS index exists: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä RAG Performance Metrics\n",
        "\n",
        "To evaluate retrieval quality and answer reliability, we measure:\n",
        "\n",
        "- **Recall@K** ‚Äì whether relevant chunks are retrieved\n",
        "- **Precision@K** ‚Äì how many retrieved chunks are actually relevant\n",
        "- **MRR (Mean Reciprocal Rank)** ‚Äì how early the correct chunk appears\n",
        "- **End-to-End Accuracy** ‚Äì correctness of final LLM answers\n",
        "- **Average Latency** ‚Äì real-world responsiveness\n",
        "\n",
        "Metrics are computed on a small labeled test set representative of the document types in the pipeline.\n"
      ],
      "metadata": {
        "id": "_OzHyxZCI6zq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "jfqpikr1qQKD",
        "outputId": "ed2a2a97-13e2-4369-d4f1-04bbfdd02258",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "FAISS index not initialized. Build index before retrieval.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-602938527.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;31m# Run Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m \u001b[0mrecall_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_recall_at_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0mmrr_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_mrr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0me2e_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_end_to_end_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-602938527.py\u001b[0m in \u001b[0;36mcompute_recall_at_k\u001b[0;34m(test_set, k)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mhits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mretrieved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_retrieval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         found = any(\n\u001b[1;32m     44\u001b[0m             \u001b[0mcontains_keyword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"gold_keywords\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-602938527.py\u001b[0m in \u001b[0;36mrun_retrieval\u001b[0;34m(question, k)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_retrieval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdoc_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FAISS index not initialized. Build index before retrieval.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     return doc_store.retriever.retrieve(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: FAISS index not initialized. Build index before retrieval."
          ]
        }
      ],
      "source": [
        "# RAG PERFORMANCE METRICS CELL\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "k = 6\n",
        "\n",
        "# Small evaluation set\n",
        "test_set = [\n",
        "    {\n",
        "        \"question\": \"What is this document about?\",\n",
        "        \"gold_keywords\": [\"agreement\", \"summary\", \"contract\", \"report\"]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are the key financial or monetary amounts mentioned?\",\n",
        "        \"gold_keywords\": [\"$\", \"amount\", \"payment\", \"total\"]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Who is the document intended for?\",\n",
        "        \"gold_keywords\": [\"client\", \"customer\", \"borrower\", \"recipient\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Helper functions\n",
        "def contains_keyword(text, keywords):\n",
        "    text = text.lower()\n",
        "    return any(k.lower() in text for k in keywords)\n",
        "\n",
        "def run_retrieval(question, k=4):\n",
        "    if doc_store.retriever.index is None:\n",
        "        raise RuntimeError(\"FAISS index not initialized. Build index before retrieval.\")\n",
        "\n",
        "    return doc_store.retriever.retrieve(\n",
        "        question,\n",
        "        k=k,\n",
        "        auto_route=True\n",
        "    )\n",
        "\n",
        "# 3. Recall@K\n",
        "def compute_recall_at_k(test_set, k=k):\n",
        "    hits = 0\n",
        "    for ex in test_set:\n",
        "        retrieved = run_retrieval(ex[\"question\"], k)\n",
        "        found = any(\n",
        "            contains_keyword(chunk.text, ex[\"gold_keywords\"])\n",
        "            for chunk, _ in retrieved\n",
        "        )\n",
        "        if found:\n",
        "            hits += 1\n",
        "    return hits / len(test_set)\n",
        "\n",
        "# MRR (Mean Reciprocal Rank)\n",
        "def compute_mrr(test_set, k=k):\n",
        "    scores = []\n",
        "    for ex in test_set:\n",
        "        retrieved = run_retrieval(ex[\"question\"], k)\n",
        "        rank = 0\n",
        "        for i, (chunk, _) in enumerate(retrieved):\n",
        "            if contains_keyword(chunk.text, ex[\"gold_keywords\"]):\n",
        "                rank = i + 1\n",
        "                break\n",
        "        scores.append(1.0 / rank if rank > 0 else 0.0)\n",
        "    return np.mean(scores)\n",
        "\n",
        "# End-to-End Accuracy\n",
        "def compute_end_to_end_accuracy(test_set):\n",
        "    correct = 0\n",
        "    for ex in test_set:\n",
        "        result = doc_store.query(ex[\"question\"], auto_route=False)\n",
        "        if contains_keyword(result[\"answer\"], ex[\"gold_keywords\"]):\n",
        "            correct += 1\n",
        "    return correct / len(test_set)\n",
        "\n",
        "# Precision@K\n",
        "def compute_precision_at_k(test_set, k=k):\n",
        "    precisions = []\n",
        "\n",
        "    for ex in test_set:\n",
        "        retrieved = run_retrieval(ex[\"question\"], k)\n",
        "\n",
        "        if len(retrieved) == 0:\n",
        "            precisions.append(0.0)\n",
        "            continue\n",
        "\n",
        "        relevant = sum(\n",
        "            contains_keyword(chunk.text, ex[\"gold_keywords\"])\n",
        "            for chunk, _ in retrieved\n",
        "        )\n",
        "\n",
        "        precisions.append(relevant / k)\n",
        "\n",
        "    return np.mean(precisions)\n",
        "\n",
        "\n",
        "# Latency Measurement\n",
        "def compute_avg_latency(test_set):\n",
        "    times = []\n",
        "    for ex in test_set:\n",
        "        start = time.time()\n",
        "        _ = doc_store.query(ex[\"question\"])\n",
        "        times.append(time.time() - start)\n",
        "    return np.mean(times)\n",
        "\n",
        "# Retrieved Chunks + Final Answer (FOR SLIDES / DEMO)\n",
        "def log_retrieved_chunks_and_answer(question, k=3):\n",
        "    retrieved = run_retrieval(question, k)\n",
        "\n",
        "    print(\"\\nüîé Retrieved Chunks (Slide Capture)\")\n",
        "    for i, (chunk, score) in enumerate(retrieved, start=1):\n",
        "        preview = chunk.text[:250].replace(\"\\n\", \" \").strip()\n",
        "        print(f\"Chunk {i} | Similarity: {score:.2f}\")\n",
        "        print(preview)\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    print(\"\\nüß† Mistral Final Answer\")\n",
        "    result = doc_store.query(question, auto_route=False)\n",
        "    print(result[\"answer\"])\n",
        "\n",
        "# Run Evaluation\n",
        "recall_k = compute_recall_at_k(test_set, k)\n",
        "mrr_k = compute_mrr(test_set, k)\n",
        "e2e_acc = compute_end_to_end_accuracy(test_set)\n",
        "precision_k = compute_precision_at_k(test_set, k)\n",
        "avg_latency = compute_avg_latency(test_set)\n",
        "\n",
        "print(\"üìä RAG Evaluation Results\")\n",
        "print(f\"Recall@{k}:        {recall_k:.2f}\")\n",
        "print(f\"Precision@{k}:     {precision_k:.2f}\")\n",
        "print(f\"MRR@{k}:           {mrr_k:.2f}\")\n",
        "print(f\"End-to-End Acc:    {e2e_acc:.2f}\")\n",
        "print(f\"Avg Latency (sec): {avg_latency:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Query1 on Slides\n",
        "log_retrieved_chunks_and_answer(\n",
        "    \"What is this document about?\",\n",
        "    k=3\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "88aZkBv9My58",
        "outputId": "17961d86-be6b-43ad-e69f-7f066156ac4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "decode: cannot decode batches with this context (calling encode() instead)\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_perf_context_print:        load time =     378.42 ms\n",
            "llama_perf_context_print: prompt eval time =       5.26 ms /     8 tokens (    0.66 ms per token,  1520.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =       7.76 ms /     9 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "decode: cannot decode batches with this context (calling encode() instead)\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_perf_context_print:        load time =     378.42 ms\n",
            "llama_perf_context_print: prompt eval time =       4.05 ms /     8 tokens (    0.51 ms per token,  1977.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =       6.56 ms /     9 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query routing error: Extra data: line 3 column 1 (char 38)\n",
            "üéØ Query routed to: Other (confidence: 0.00)\n",
            "\n",
            "üîé Retrieved Chunks (Slide Capture)\n",
            "Chunk 1 | Similarity: 0.04\n",
            "Your actual rate, payment, and cost could be higher. Get an official Loan Estimate before choosing a loan. Fee Details and Summary Applicants: Application No: Date Prepared: Loan Program: Prepared By: THIS IS NOT A GOOD FAITH ESTIMATE (GFE). This \"Fe\n",
            "--------------------------------------------------\n",
            "Chunk 2 | Similarity: 0.01\n",
            "frm (09/2015) FEES WORKSHEET John Q. Smith / Mary A. Smith samplesmith 10/05/2015 30 YEAR FIXED -Purchase XYZ Lender $ 380,000 4.250 % 360 / 360 mths 475,000.00 1,121.53 4,520.00 380,000.00 Cash Deposit 5,000.00 needed to close 95,641.53 1,869.37 39.\n",
            "--------------------------------------------------\n",
            "Chunk 3 | Similarity: 0.00\n",
            "00 Lender's Title Insurance Borrower $ 650.00 Title - Courier Fee Settlement Agent Borrower $ 50.00 Electronic Document Delivery FeeSettlement Agent Borrower $ 50.00 Pest Inspection Fee PEST CONTROL Borrower $ 50.00 Home Inspection HI COMPANY Borrowe\n",
            "--------------------------------------------------\n",
            "\n",
            "üß† Mistral Final Answer\n",
            "The document is a Mortgage Fees Worksheet. It provides an estimate of various fees and charges that the borrowers, John Q. Smith and Mary A. Smith, would need to pay during the mortgage transaction. This includes origination charges, other charges, and monthly payments. The document does not contain a Good Faith Estimate (GFE), as indicated at the top of the page. The information is presented on pages 1-1 of the document.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Query2 on Slides\n",
        "log_retrieved_chunks_and_answer(\n",
        "    \"What are the key financial or monetary amounts mentioned?\",\n",
        "    k=3\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7I0J6yQeR7ZG",
        "outputId": "85f0b393-7f1c-4fac-b840-cec70eae4fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "decode: cannot decode batches with this context (calling encode() instead)\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_perf_context_print:        load time =     378.42 ms\n",
            "llama_perf_context_print: prompt eval time =       4.31 ms /    12 tokens (    0.36 ms per token,  2785.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =       5.79 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "decode: cannot decode batches with this context (calling encode() instead)\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_perf_context_print:        load time =     378.42 ms\n",
            "llama_perf_context_print: prompt eval time =       4.44 ms /    12 tokens (    0.37 ms per token,  2705.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =       6.23 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query routing error: Extra data: line 2 column 1 (char 39)\n",
            "üéØ Query routed to: Other (confidence: 0.00)\n",
            "\n",
            "üîé Retrieved Chunks (Slide Capture)\n",
            "Chunk 1 | Similarity: 0.11\n",
            "Your actual rate, payment, and cost could be higher. Get an official Loan Estimate before choosing a loan. Fee Details and Summary Applicants: Application No: Date Prepared: Loan Program: Prepared By: THIS IS NOT A GOOD FAITH ESTIMATE (GFE). This \"Fe\n",
            "--------------------------------------------------\n",
            "Chunk 2 | Similarity: 0.01\n",
            "frm (09/2015) FEES WORKSHEET John Q. Smith / Mary A. Smith samplesmith 10/05/2015 30 YEAR FIXED -Purchase XYZ Lender $ 380,000 4.250 % 360 / 360 mths 475,000.00 1,121.53 4,520.00 380,000.00 Cash Deposit 5,000.00 needed to close 95,641.53 1,869.37 39.\n",
            "--------------------------------------------------\n",
            "Chunk 3 | Similarity: 0.00\n",
            "Payslip Pay Date : 2025/07/17 Working Days : 26 Employee Name : James Bond Employee ID : 007 Earnings Amount Deductions Amount Basic Pay 8000 Tax 800 Allowance 500 Overtime 300        Total Earnings 8800 Total Deductions 800     Net Pay 8000 0 Employ\n",
            "--------------------------------------------------\n",
            "\n",
            "üß† Mistral Final Answer\n",
            "The key financial or monetary amounts mentioned in the context include:\n",
            "1. Loan amount: $380,000 (Mortgage Document, Pages 1-1)\n",
            "2. Proposed monthly mortgage payment: $1,869.37 (Mortgage Document, Pages 1-1)\n",
            "3. Cash needed to close: $95,641.53 (Mortgage Document, Pages 1-1)\n",
            "4. Origination charges: $550 for underwriting fee, $75 for wire transfer fee, $445 for administration fee, totaling $1,070 (Mortgage Document, Pages 1-1)\n",
            "5. Appraisal fee: $525 (Mortgage Document, Pages 1-1)\n",
            "6. Credit report fee: Unknown (Mortgage Document, Pages 1-1)\n",
            "7. Basic pay: $8,000 (Payslip, Pages 2-3)\n",
            "8. Net pay: $8,000 (Payslip, Pages 2-3)\n",
            "\n",
            "Note: The context does not provide enough information to determine the credit report fee or the total earnings and deductions for the second payslip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Current Limitations & Next Steps\n",
        "\n",
        "**Known limitations**\n",
        "- OCR accuracy degrades on low-resolution or handwritten documents\n",
        "- Table extraction may lose row alignment\n",
        "- Retrieval precision can drop on very similar clauses\n",
        "\n",
        "**Next steps**\n",
        "- Add a cross-encoder reranker\n",
        "- Table-aware parsing for financial line items\n",
        "- Multilingual OCR support\n"
      ],
      "metadata": {
        "id": "pxdQyDqIIn_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üñ•Ô∏è Interactive Demo (Optional)\n",
        "\n",
        "The following Gradio interface allows users to upload PDFs, inspect document structure, and interact with the RAG pipeline in real time.\n"
      ],
      "metadata": {
        "id": "8st_ikn3Iu9T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LJJxZwTpN_pQ",
        "outputId": "cc64ce24-834e-4c93-dc0c-9a0a4ec42042",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a7b523fcce2a0a2ed0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a7b523fcce2a0a2ed0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìñ Starting PDF extraction and analysis...\n",
            "‚úÖ Extracted 4 pages\n",
            "üß† Analyzing document structure...\n",
            "  Page 0: New document detected - Resume\n",
            "  Page 1: New document detected - Mortgage Document\n",
            "  Page 2: New document detected - Payslip\n",
            "‚úÖ Identified 3 logical documents\n",
            "   - Resume: Pages 0-0\n",
            "   - Mortgage Document: Pages 1-1\n",
            "   - Payslip: Pages 2-3\n",
            "üìÑ Resume: Created 1 chunks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Mortgage Document: Created 3 chunks\n",
            "üìÑ Payslip: Created 1 chunks\n",
            "üî® Building vector indices...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Indexed 5 chunks across 3 document types\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query routing error: Extra data: line 3 column 1 (char 40)\n",
            "üéØ Query routed to: Other (confidence: 0.00)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query routing error: Extra data: line 3 column 1 (char 39)\n",
            "üéØ Query routed to: Other (confidence: 0.00)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Query routed to: Bank Statement (confidence: 0.95)\n"
          ]
        }
      ],
      "source": [
        "demo = create_interface()\n",
        "demo.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}